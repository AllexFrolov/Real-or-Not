{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_column(dataframe):\n",
    "    hashtags = []\n",
    "    for text in dataframe.text:\n",
    "        result = re.findall('#\\w+', text)\n",
    "        if result != []:\n",
    "            result = [w[1:].lower() for w in result]\n",
    "            hashtags.append(' '.join(result))\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_texts(texts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    result = []\n",
    "    for t in texts:\n",
    "        lemmatized_words = []\n",
    "        t = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "                  'url', t)\n",
    "        #t = re.sub('\\!+', '', t)\n",
    "        #t = re.sub('\\?+', '', t)\n",
    "        #t = re.sub('\\d+[\\:|\\.]?\\d*\\s')\n",
    "        t = re.sub('\\d+', '', t)\n",
    "        tokens = re.findall('''\\d+,\\d+|\\w+'\\w+|#?\\w+-?\\w+|\\w+\\*+\\w+''', t)\n",
    "        \n",
    "        if tokens == []:\n",
    "            print(t)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token.lower() not in stop_words:\n",
    "                lemmatized_words.append(lemmatizer.lemmatize(token).lower())\n",
    "        result.append(' '.join(lemmatized_words).replace('#', ''))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lemmatized_texts = lemmatize_texts(data.text)\n",
    "all_lemmatized_tokens = [w for t in all_lemmatized_texts for w in t.split(' ')]\n",
    "print('Total words: ', len(all_lemmatized_tokens))\n",
    "print('Unique_words: ', len(set(all_lemmatized_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in dataset\n",
    "freq = nltk.probability.FreqDist(all_lemmatized_tokens)\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in real tweets\n",
    "real_tweets = data[data.target == 1].text\n",
    "real_tweets = lemmatize_texts(real_tweets)\n",
    "freq_real = nltk.probability.FreqDist([w for t in real_tweets for w in t.split(' ')])\n",
    "freq_real.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in fake tweets\n",
    "fake_tweets = data[data.target == 0].text\n",
    "fake_tweets = lemmatize_texts(fake_tweets)\n",
    "freq_fake = nltk.probability.FreqDist([w for t in fake_tweets for w in t.split(' ')])\n",
    "freq_fake.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_lemmatized_texts, \n",
    "                                                    data.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize texts\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), tokenizer=tokenizer)\n",
    "train = vectorizer.fit_transform(X_train)\n",
    "test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM (accuracy: 0.78)\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0, tol=1e-5, max_iter=5000)\n",
    "clf.fit(train, y_train)\n",
    "clf.score(test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
