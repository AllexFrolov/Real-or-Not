{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    is_in_colab = True\n",
    "except:\n",
    "    is_in_colab = False\n",
    "\n",
    "if is_in_colab:\n",
    "    drive.mount('/content/drive')\n",
    "    data_folder = r'/content/drive/My Drive/Colab/Real-or-Not/data/'\n",
    "else:\n",
    "    data_folder = r'./data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data = pd.read_csv(data_folder + '/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_column(dataframe):\n",
    "    hashtags = []\n",
    "    for text in dataframe.text:\n",
    "        result = re.findall('#\\w+', text)\n",
    "        if result != []:\n",
    "            result = [w[1:].lower() for w in result]\n",
    "            hashtags.append(' '.join(result))\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_texts(texts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    result = []\n",
    "    for t in texts:\n",
    "        lemmatized_words = []\n",
    "        t = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "                  'url', t)\n",
    "        #t = re.sub('\\!+', '', t)\n",
    "        #t = re.sub('\\?+', '', t)\n",
    "        #t = re.sub('\\d+[\\:|\\.]?\\d*\\s')\n",
    "        t = re.sub('\\d+', '', t)\n",
    "        tokens = re.findall('''\\d+,\\d+|\\w+'\\w+|#?\\w+-?\\w+|\\w+\\*+\\w+''', t)\n",
    "        \n",
    "        if tokens == []:\n",
    "            print(t)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token.lower() not in stop_words:\n",
    "                lemmatized_words.append(lemmatizer.lemmatize(token).lower())\n",
    "        result.append(' '.join(lemmatized_words).replace('#', ''))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  72070\n",
      "Unique_words:  15827\n"
     ]
    }
   ],
   "source": [
    "all_lemmatized_texts = lemmatize_texts(data.text)\n",
    "all_lemmatized_tokens = [w for t in all_lemmatized_texts for w in t.split(' ')]\n",
    "print('Total words: ', len(all_lemmatized_tokens))\n",
    "print('Unique_words: ', len(set(all_lemmatized_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('url', 4114),\n",
       " ('รป_', 349),\n",
       " ('like', 347),\n",
       " ('fire', 339),\n",
       " ('amp', 327),\n",
       " (\"i'm\", 248),\n",
       " ('get', 248),\n",
       " ('new', 217),\n",
       " ('one', 201),\n",
       " ('news', 199),\n",
       " ('people', 198),\n",
       " ('disaster', 159),\n",
       " ('video', 158),\n",
       " ('emergency', 156),\n",
       " ('time', 147),\n",
       " ('body', 145),\n",
       " ('day', 141),\n",
       " ('police', 141),\n",
       " ('year', 133),\n",
       " ('would', 132)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common words in dataset\n",
    "freq = nltk.probability.FreqDist(all_lemmatized_tokens)\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('url', 2228),\n",
       " ('fire', 253),\n",
       " ('รป_', 172),\n",
       " ('news', 142),\n",
       " ('amp', 130),\n",
       " ('disaster', 120),\n",
       " ('california', 110),\n",
       " ('suicide', 109),\n",
       " ('police', 107),\n",
       " ('people', 106)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common words in real tweets\n",
    "real_tweets = data[data.target == 1].text\n",
    "real_tweets = lemmatize_texts(real_tweets)\n",
    "freq_real = nltk.probability.FreqDist([w for t in real_tweets for w in t.split(' ')])\n",
    "freq_real.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('url', 1886),\n",
       " ('like', 254),\n",
       " (\"i'm\", 203),\n",
       " ('amp', 197),\n",
       " ('get', 181),\n",
       " ('รป_', 177),\n",
       " ('new', 163),\n",
       " ('one', 132),\n",
       " ('body', 114),\n",
       " ('would', 98)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common words in fake tweets\n",
    "fake_tweets = data[data.target == 0].text\n",
    "fake_tweets = lemmatize_texts(fake_tweets)\n",
    "freq_fake = nltk.probability.FreqDist([w for t in fake_tweets for w in t.split(' ')])\n",
    "freq_fake.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(all_lemmatized_texts, \n",
    "                                                    data.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, train_size = 0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize texts\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), tokenizer=tokenizer)\n",
    "train = vectorizer.fit_transform(X_train)\n",
    "val = vectorizer.transform(X_val)\n",
    "test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7783251231527094"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train SVM (accuracy: 0.78)\n",
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC(random_state=0)\n",
    "# svc_parameters = {'tol' : [1e-7, 1e-6, 1e-5], \n",
    "#                   'max_iter': np.arange(400, 1000, 200)\n",
    "#                  }\n",
    "# grids_svc = GridSearchCV(svc, svc_parameters, n_jobs=-1, cv=5)\n",
    "svc.fit(train, y_train)\n",
    "svc.score(val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grids_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grids_clf.best_score_, grids_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7898193760262726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(random_state=42, \n",
    "                                n_estimators=71, \n",
    "                                min_samples_leaf=2, \n",
    "                                max_features=300, \n",
    "                                oob_score=True)\n",
    "forest.fit(train, y_train)\n",
    "# forest_params = {'max_features': np.arange(100, 1000, 100)}\n",
    "# grid_forest = GridSearchCV(forest, forest_params, n_jobs=-1, cv=5)\n",
    "print(forest.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7824302134646962"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 'url', importance=0.038834826701685504\n",
      "Feature 'fire', importance=0.014192495093343922\n",
      "Feature 'hiroshima', importance=0.010890804969129145\n",
      "Feature 'killed', importance=0.008864163516005616\n",
      "Feature 'suicide', importance=0.007806427495297494\n",
      "Feature 'california', importance=0.007719488555323708\n",
      "Feature 'train', importance=0.006713296992594389\n",
      "Feature 'earthquake', importance=0.006668208361033888\n",
      "Feature 'japan', importance=0.00631094108572016\n",
      "Feature 'storm', importance=0.0056789492384608856\n",
      "Feature 'bombing', importance=0.005557711470771259\n",
      "Feature 'attack', importance=0.005226616789743979\n",
      "Feature 'wildfire', importance=0.005109212224453291\n",
      "Feature 'evacuated', importance=0.004866605547471444\n",
      "Feature 'disaster', importance=0.00483162619307037\n",
      "Feature 'mh', importance=0.004543441517120972\n",
      "Feature 'massacre', importance=0.004405012795758994\n",
      "Feature 'terrorist', importance=0.004375640380339555\n",
      "Feature 'atomic', importance=0.004348805204204863\n",
      "Feature 'sinkhole', importance=0.004213986033487731\n"
     ]
    }
   ],
   "source": [
    "importance = sorted(zip(vectorizer.get_feature_names(), forest.feature_importances_), key=lambda x: x[1], reverse=True)\n",
    "for imp in importance[:20]: print(\"Feature '{}', importance={}\".format(*imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# grid_forest.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# grid_forest.best_score_, grid_forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (0.7779967159277504, {'n_estimators': 71})\n",
    "# (0.790311986863711, {'max_features': 'auto'})\n",
    "# (0.7957307060755336, {'max_features': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME',\n",
       "                   base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=None,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort=False,\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=0.08, n_estimators=7, random_state=42)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "boost = AdaBoostClassifier(base_estimator=sklearn.tree.DecisionTreeClassifier(),\n",
    "                           random_state=42, algorithm='SAMME', learning_rate=0.08,\n",
    "                           n_estimators = 7\n",
    "                          )\n",
    "boost_params = {'n_estimators': np.arange(1, 10),\n",
    "                'learning_rate': np.arange(0.01, 0.1, 0.01)\n",
    "                }\n",
    "# grid_boost = GridSearchCV(boost, boost_params, n_jobs=-1, cv=5)\n",
    "boost.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7216748768472906"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost.score(val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_boost.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7489737274220033, {'learning_rate': 0.08, 'n_estimators': 7})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid_boost.best_score_, grid_boost.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device('cuda:0')\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    from torch import FloatTensor, LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data, batch_size, shuffle=False):\n",
    "    features = data[0]\n",
    "    target = data[1]\n",
    "    n_samples = features.shape[0]\n",
    "    \n",
    "    indices = np.arange(n_samples)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch_indices = indices[start : end]\n",
    "        X_batch = features[batch_indices].toarray()\n",
    "        y_batch = target.values[batch_indices]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fit(model, loss_function, train_data=None, val_data=None, optimizer=None,\n",
    "        epoch_count=1, batch_size=1, scheduler=None, alpha=1):\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    best_model = None\n",
    "    for epoch in range(epoch_count):\n",
    "            name_prefix = '[{} / {}] '.format(epoch + 1, epoch_count)\n",
    "            epoch_train_score = 0\n",
    "            epoch_val_score = 0\n",
    "            \n",
    "            if train_data:\n",
    "                epoch_train_score = do_epoch(model, loss_function, train_data, batch_size, \n",
    "                                              optimizer, name_prefix + 'Train:', alpha=alpha\n",
    "                                            )\n",
    "                train_history.append(epoch_train_score)\n",
    "\n",
    "            if val_data:\n",
    "                name = '  Val:'\n",
    "                if not train_data:\n",
    "                    name = ' Test:'\n",
    "                epoch_val_score = do_epoch(model, loss_function, val_data, batch_size, \n",
    "                                             optimizer=None, name=name_prefix + name, alpha=alpha\n",
    "                                          )\n",
    "                \n",
    "                val_history.append(epoch_val_score)\n",
    "                if scheduler:\n",
    "                    scheduler.step(epoch_val_score)\n",
    "            elif scheduler:\n",
    "                scheduler.step(epoch_train_score)\n",
    "\n",
    "    return train_history, val_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loss_function, data, batch_size, optimizer=None, name=None, alpha=1):\n",
    "    \"\"\"\n",
    "       ะะตะฝะตัะฐัะธั ะพะดะฝะพะน ัะฟะพัะธ\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    epoch_loss = 0\n",
    "   \n",
    "    batch_count = int(data[0].shape[0] / batch_size)\n",
    "   \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batch_count) as progress_bar:               \n",
    "            for ind, (X, y) in enumerate(data_loader(data, batch_size)):\n",
    "                X_batch, y_batch = FloatTensor(X).to(device), LongTensor(y).to(device)\n",
    "                \n",
    "                prediction = model(X_batch)\n",
    "                \n",
    "                loss = loss_function(prediction, y_batch)\n",
    "                \n",
    "                for param in model.children():\n",
    "                    if type(param) == nn.Linear:\n",
    "                        loss += alpha * torch.abs(param.weight).sum()\n",
    "                        \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                true_indices = torch.argmax(prediction, dim=1)\n",
    "                correct_samples = torch.sum(true_indices == y_batch).cpu().numpy()\n",
    "                accuracy += correct_samples / y_batch.shape[0]\n",
    "                if is_train:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('Epoch {} - accuracy: {:.2f}, loss {:.2f}'.format(\n",
    "                    name, (accuracy / (ind+1)), epoch_loss / (ind+1))\n",
    "                )\n",
    "            \n",
    "            accuracy /= (ind + 1)\n",
    "            epoch_loss /= (ind + 1) \n",
    "            progress_bar.set_description(f'Epoch {name} - accuracy: {accuracy:.2f}, loss: {epoch_loss:.2f}')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LinearNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        self.model.eval()\n",
    "        output = pd.DataFrame()\n",
    "        for ind in range(inputs.shape[0]):\n",
    "            X = FloatTensor(inputs[ind].toarray())\n",
    "            predict = self.model(X)\n",
    "            true_indices = torch.argmax(predict, dim=1).detach().cpu().numpy()\n",
    "            output.loc[ind, 'target'] = true_indices\n",
    "        return output.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fit settings\n",
    "batch_size = 100\n",
    "epoch_count = 10\n",
    "\n",
    "# optim settings\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.5\n",
    "alpha = 0\n",
    "\n",
    "# model settings\n",
    "linear1_out = int(train.shape[1]**0.5)\n",
    "output = 2\n",
    "dropout = 0.3\n",
    "\n",
    "# scheduler settings\n",
    "factor = 0.5\n",
    "patience = 3\n",
    "threshold = 1e-2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(train.shape[1], linear1_out),\n",
    "                      nn.BatchNorm1d(linear1_out),\n",
    "#                       nn.Dropout(p=dropout, inplace=True),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.Linear(linear1_out, output),\n",
    "                      nn.ReLU(inplace=True)\n",
    "                     ).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "                        model.parameters(),\n",
    "                        lr=learning_rate, \n",
    "                        weight_decay=weight_decay\n",
    "                    )\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=factor, \n",
    "                              patience=patience, verbose=True, threshold=threshold\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "                        model.parameters(),\n",
    "                        lr=learning_rate, \n",
    "                        weight_decay=weight_decay\n",
    "                    )\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=factor, \n",
    "                              patience=patience, verbose=True, threshold=threshold\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 10] Train: - accuracy: 0.74, loss: 0.54: : 49it [00:01, 26.10it/s]                     \n",
      "Epoch [1 / 10]   Val: - accuracy: 0.76, loss: 0.64: : 13it [00:00, 39.82it/s]                     \n",
      "Epoch [2 / 10] Train: - accuracy: 0.76, loss: 0.53: : 49it [00:01, 26.56it/s]                     \n",
      "Epoch [2 / 10]   Val: - accuracy: 0.75, loss: 0.53: : 13it [00:00, 39.10it/s]                     \n",
      "Epoch [3 / 10] Train: - accuracy: 0.77, loss: 0.52: : 49it [00:01, 27.18it/s]                     \n",
      "Epoch [3 / 10]   Val: - accuracy: 0.76, loss: 0.53: : 13it [00:00, 34.67it/s]                     \n",
      "Epoch [4 / 10] Train: - accuracy: 0.77, loss: 0.52: : 49it [00:01, 28.23it/s]                     \n",
      "Epoch [4 / 10]   Val: - accuracy: 0.75, loss: 0.53: : 13it [00:00, 31.04it/s]                     \n",
      "Epoch [5 / 10] Train: - accuracy: 0.78, loss: 0.52: : 49it [00:01, 29.07it/s]                     \n",
      "Epoch [5 / 10]   Val: - accuracy: 0.75, loss: 0.53: : 13it [00:00, 29.99it/s]                     \n",
      "Epoch [6 / 10] Train: - accuracy: 0.73, loss 0.55:   8%|โ         | 4/48 [00:00<00:01, 24.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     5: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6 / 10] Train: - accuracy: 0.77, loss: 0.52: : 49it [00:02, 23.58it/s]                     \n",
      "Epoch [6 / 10]   Val: - accuracy: 0.77, loss: 0.50: : 13it [00:00, 30.27it/s]                     \n",
      "Epoch [7 / 10] Train: - accuracy: 0.76, loss: 0.53: : 49it [00:01, 29.28it/s]                     \n",
      "Epoch [7 / 10]   Val: - accuracy: 0.77, loss: 0.51: : 13it [00:00, 30.21it/s]                     \n",
      "Epoch [8 / 10] Train: - accuracy: 0.77, loss: 0.53: : 49it [00:01, 28.74it/s]                     \n",
      "Epoch [8 / 10]   Val: - accuracy: 0.77, loss: 0.51: : 13it [00:00, 30.52it/s]                     \n",
      "Epoch [9 / 10] Train: - accuracy: 0.78, loss: 0.53: : 49it [00:02, 23.82it/s]                     \n",
      "Epoch [9 / 10]   Val: - accuracy: 0.76, loss: 0.52: : 13it [00:00, 29.50it/s]                     \n",
      "Epoch [10 / 10] Train: - accuracy: 0.78, loss: 0.53: : 49it [00:02, 23.28it/s]                     \n",
      "Epoch [10 / 10]   Val: - accuracy: 0.77, loss: 0.51: : 13it [00:00, 29.42it/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    10: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _ = fit(model, loss_function, train_data=(train, y_train), \n",
    "           optimizer=optimizer, batch_size=100, epoch_count=epoch_count,\n",
    "           alpha = alpha, val_data=(val, y_val), scheduler=scheduler\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(svc)\n",
    "models.append(forest)\n",
    "# models.append(boost)\n",
    "models.append(NNModel(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(models, data):\n",
    "    predicts = pd.DataFrame()\n",
    "    for i, model in enumerate(models):\n",
    "        predicts[i] = model.predict(data)\n",
    "    result = predicts.apply(lambda row: row.value_counts().index[0], axis=1)\n",
    "    return result.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7873563218390804"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ensemble(models, val), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# choose your best model\n",
    "final_model = NNModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def submission(model, vectorizer, file_name=\"submission.csv\"):\n",
    "    test_data = pd.read_csv(data_folder + \"/test.csv\")\n",
    "    all_lemmatized_texts = lemmatize_texts(test_data.text)\n",
    "    test = vectorizer.transform(all_lemmatized_texts)\n",
    "    submit = pd.DataFrame()\n",
    "    submit['id'] = test_data['id']\n",
    "    submit['target'] = final_model.predict(test)\n",
    "    submit['target'] = submit['target'].astype('int')\n",
    "    submit.to_csv(data_folder + file_name, index=False)\n",
    "    \n",
    "submission(final_model, vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
