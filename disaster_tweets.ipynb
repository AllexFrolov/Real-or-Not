{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import sklearn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_column(dataframe):\n",
    "    hashtags = []\n",
    "    for text in dataframe.text:\n",
    "        result = re.findall('#\\w+', text)\n",
    "        if result != []:\n",
    "            result = [w[1:].lower() for w in result]\n",
    "            hashtags.append(' '.join(result))\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_texts(texts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    result = []\n",
    "    for t in texts:\n",
    "        lemmatized_words = []\n",
    "        t = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "                  'url', t)\n",
    "        #t = re.sub('\\!+', '', t)\n",
    "        #t = re.sub('\\?+', '', t)\n",
    "        #t = re.sub('\\d+[\\:|\\.]?\\d*\\s')\n",
    "        t = re.sub('\\d+', '', t)\n",
    "        tokens = re.findall('''\\d+,\\d+|\\w+'\\w+|#?\\w+-?\\w+|\\w+\\*+\\w+''', t)\n",
    "        \n",
    "        if tokens == []:\n",
    "            print(t)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token.lower() not in stop_words:\n",
    "                lemmatized_words.append(lemmatizer.lemmatize(token).lower())\n",
    "        result.append(' '.join(lemmatized_words).replace('#', ''))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  72070\n",
      "Unique_words:  15827\n"
     ]
    }
   ],
   "source": [
    "all_lemmatized_texts = lemmatize_texts(data.text)\n",
    "all_lemmatized_tokens = [w for t in all_lemmatized_texts for w in t.split(' ')]\n",
    "print('Total words: ', len(all_lemmatized_tokens))\n",
    "print('Unique_words: ', len(set(all_lemmatized_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('url', 4114),\n",
       " ('รป_', 349),\n",
       " ('like', 347),\n",
       " ('fire', 339),\n",
       " ('amp', 327),\n",
       " (\"i'm\", 248),\n",
       " ('get', 248),\n",
       " ('new', 217),\n",
       " ('one', 201),\n",
       " ('news', 199),\n",
       " ('people', 198),\n",
       " ('disaster', 159),\n",
       " ('video', 158),\n",
       " ('emergency', 156),\n",
       " ('time', 147),\n",
       " ('body', 145),\n",
       " ('day', 141),\n",
       " ('police', 141),\n",
       " ('year', 133),\n",
       " ('would', 132)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common words in dataset\n",
    "freq = nltk.probability.FreqDist(all_lemmatized_tokens)\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('url', 2228),\n",
       " ('fire', 253),\n",
       " ('รป_', 172),\n",
       " ('news', 142),\n",
       " ('amp', 130),\n",
       " ('disaster', 120),\n",
       " ('california', 110),\n",
       " ('suicide', 109),\n",
       " ('police', 107),\n",
       " ('people', 106)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common words in real tweets\n",
    "real_tweets = data[data.target == 1].text\n",
    "real_tweets = lemmatize_texts(real_tweets)\n",
    "freq_real = nltk.probability.FreqDist([w for t in real_tweets for w in t.split(' ')])\n",
    "freq_real.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('url', 1886),\n",
       " ('like', 254),\n",
       " (\"i'm\", 203),\n",
       " ('amp', 197),\n",
       " ('get', 181),\n",
       " ('รป_', 177),\n",
       " ('new', 163),\n",
       " ('one', 132),\n",
       " ('body', 114),\n",
       " ('would', 98)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common words in fake tweets\n",
    "fake_tweets = data[data.target == 0].text\n",
    "fake_tweets = lemmatize_texts(fake_tweets)\n",
    "freq_fake = nltk.probability.FreqDist([w for t in fake_tweets for w in t.split(' ')])\n",
    "freq_fake.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_lemmatized_texts, \n",
    "                                                    data.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize texts\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), tokenizer=tokenizer)\n",
    "train = vectorizer.fit_transform(X_train)\n",
    "test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM (accuracy: 0.78)\n",
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC(random_state=0)\n",
    "# svc_parameters = {'tol' : [1e-7, 1e-6, 1e-5], \n",
    "#                   'max_iter': np.arange(400, 1000, 200)\n",
    "#                  }\n",
    "# grids_svc = GridSearchCV(svc, svc_parameters, n_jobs=-1, cv=5)\n",
    "svc.fit(train, y_train)\n",
    "# clf.score(test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\svetl\\for project\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 loss='squared_hinge', max_iter=1000,\n",
       "                                 multi_class='ovr', penalty='l2',\n",
       "                                 random_state=0, tol=0.0001, verbose=0),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_iter': array([1000, 2000]),\n",
       "                         'tol': array([5.0e-07, 1.0e-06, 1.5e-06, 2.0e-06, 2.5e-06, 3.0e-06, 3.5e-06,\n",
       "       4.0e-06, 4.5e-06, 5.0e-06, 5.5e-06, 6.0e-06, 6.5e-06, 7.0e-06,\n",
       "       7.5e-06, 8.0e-06, 8.5e-06, 9.0e-06, 9.5e-06, 1.0e-05])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grids_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7829228243021347, {'max_iter': 1000, 'tol': 5e-07})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grids_clf.best_score_, grids_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8003284072249589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(random_state=42, \n",
    "                                n_estimators=71, \n",
    "                                min_samples_leaf=2, \n",
    "                                max_features=300, \n",
    "                                oob_score=True)\n",
    "forest.fit(train, y_train)\n",
    "# forest_params = {'max_features': np.arange(100, 1000, 100)}\n",
    "# grid_forest = GridSearchCV(forest, forest_params, n_jobs=-1, cv=5)\n",
    "print(forest.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# grid_forest.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# grid_forest.best_score_, grid_forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (0.7779967159277504, {'n_estimators': 71})\n",
    "# (0.790311986863711, {'max_features': 'auto'})\n",
    "# (0.7957307060755336, {'max_features': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "boost = AdaBoostClassifier(base_estimator=sklearn.tree.DecisionTreeClassifier(max_depth=10),\n",
    "                           random_state=42, algorithm='SAMME')\n",
    "boost_params = {'n_estimators': np.arange(1, 10),\n",
    "                'learning_rate': np.arange(0.01, 0.1, 0.01)\n",
    "                }\n",
    "grid_boost = GridSearchCV(boost, boost_params, n_jobs=-1, cv=5)\n",
    "# boost.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=AdaBoostClassifier(algorithm='SAMME',\n",
       "                                          base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                                                criterion='gini',\n",
       "                                                                                max_depth=10,\n",
       "                                                                                max_features=None,\n",
       "                                                                                max_leaf_nodes=None,\n",
       "                                                                                min_impurity_decrease=0.0,\n",
       "                                                                                min_impurity_split=None,\n",
       "                                                                                min_samples_leaf=1,\n",
       "                                                                                min_samples_split=2,\n",
       "                                                                                min_weight_fraction_leaf=0.0,\n",
       "                                                                                presort=False,\n",
       "                                                                                random_state=None,\n",
       "                                                                                splitter='best'),\n",
       "                                          learning_rate=1.0, n_estimators=50,\n",
       "                                          random_state=42),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]),\n",
       "                         'n_estimators': array([1, 2, 3, 4, 5, 6, 7, 8, 9])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_boost.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6609195402298851, {'learning_rate': 0.08, 'n_estimators': 9})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_boost.best_score_, grid_boost.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device('cuda:0')\n",
    "    from torch.cuda import FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    from torch import FloatTensor\n",
    "    \n",
    "try:\n",
    "    from google.colab import drive\n",
    "    is_in_colab = True\n",
    "except:\n",
    "    is_in_colab = False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LinearNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fit settings\n",
    "batch_size = 50\n",
    "epoch_count = 10\n",
    "\n",
    "# optim settings\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0\n",
    "\n",
    "# model settings\n",
    "linear1_out = int(train.shape[1]**0.5)\n",
    "output = 2\n",
    "\n",
    "# scheduler settings\n",
    "factor = 0.1\n",
    "patience = 2\n",
    "threshold = 1e-2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(train.shape[1], linear1_out),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.Linear(linear1_out, output),\n",
    "                      nn.ReLU(inplace=True)\n",
    "                     ).to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "                        model.parameters(),\n",
    "                        lr=learning_rate, \n",
    "                        weight_decay=weight_decay\n",
    "                    )\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=factor, \n",
    "                              patience=patience, verbose=True, threshold=threshold\n",
    "                              )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}