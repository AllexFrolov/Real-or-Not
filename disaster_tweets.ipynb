{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device('cuda:0')\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    \n",
    "try:\n",
    "    from google.colab import drive\n",
    "    is_in_colab = True\n",
    "except:\n",
    "    is_in_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    is_in_colab = True\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "except:\n",
    "    is_in_colab = False\n",
    "\n",
    "if is_in_colab:\n",
    "    drive.mount('/content/drive')\n",
    "    data_folder = r'/content/drive/My Drive/Colab/Real-or-Not/data/'\n",
    "else:\n",
    "    data_folder = r'./data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data = pd.read_csv(data_folder + '/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_column(dataframe):\n",
    "    hashtags = []\n",
    "    for text in dataframe.text:\n",
    "        result = re.findall('#\\w+', text)\n",
    "        if result != []:\n",
    "            result = [w[1:].lower() for w in result]\n",
    "            hashtags.append(' '.join(result))\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_texts(texts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    result = []\n",
    "    for t in texts:\n",
    "        lemmatized_words = []\n",
    "        t = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "                  'url', t)\n",
    "        #t = re.sub('\\!+', '', t)\n",
    "        #t = re.sub('\\?+', '', t)\n",
    "        #t = re.sub('\\d+[\\:|\\.]?\\d*\\s')\n",
    "#         t = re.sub('\\d+', '', t)\n",
    "        tokens = re.findall('''\\d+,\\d+|\\w+'\\w+|#?\\w+-?\\w+|\\w+\\*+\\w+''', t)\n",
    "        \n",
    "        if tokens == []:\n",
    "            print(t)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token.lower() not in stop_words:\n",
    "                lemmatized_words.append(lemmatizer.lemmatize(token).lower())\n",
    "        result.append(' '.join(lemmatized_words).replace('#', ''))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['keyword']].fillna('-9999')\n",
    "features['location'] = data['location'].fillna('-9999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['text'] = pd.Series(lemmatize_texts(data.text), name='text')\n",
    "all_lemmatized_tokens = [w for t in features['text'] for w in t.split(' ')]\n",
    "print('Total words: ', len(all_lemmatized_tokens))\n",
    "print('Unique_words: ', len(set(all_lemmatized_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in dataset\n",
    "freq = nltk.probability.FreqDist(all_lemmatized_tokens)\n",
    "# freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in real tweets\n",
    "real_tweets = data[data.target == 1].text\n",
    "real_tweets = lemmatize_texts(real_tweets)\n",
    "freq_real = nltk.probability.FreqDist([w for t in real_tweets for w in t.split(' ')])\n",
    "# freq_real.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in fake tweets\n",
    "fake_tweets = data[data.target == 0].text\n",
    "fake_tweets = lemmatize_texts(fake_tweets)\n",
    "freq_fake = nltk.probability.FreqDist([w for t in fake_tweets for w in t.split(' ')])\n",
    "# freq_fake.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(features, data.target, \n",
    "                                                            test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, train_size = 0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'training' (tf-)idf vectorizer.\n",
    "tf_idf = TfidfVectorizer(stop_words=stop_words,\n",
    "                         smooth_idf=False)\n",
    "tf_idf.fit(X_train['text'] + X_train['keyword'] + X_train['location'])\n",
    "#getting idfs\n",
    "idfs = tf_idf.idf_\n",
    "#sorting out too rare and too common words\n",
    "lower_thresh = 5.\n",
    "upper_thresh = 8.\n",
    "mask = (idfs < lower_thresh) | (idfs > upper_thresh)\n",
    "\n",
    "bad_words = np.array(tf_idf.get_feature_names())[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "f, ax = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "sns.distplot(idfs, ax=ax[0]); \n",
    "sns.boxplot(idfs, ax=ax[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style('darkgrid')\n",
    "f, ax = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "sns.distplot(idfs[~mask], ax=ax[0]); \n",
    "sns.boxplot(idfs[~mask], ax=ax[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf with remove stop-words and bad_words\n",
    "tf = TfidfVectorizer(stop_words=stop_words.union(bad_words),\n",
    "                         smooth_idf=False, ngram_range=(1, 1), \n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorize texts\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), tokenizer=tokenizer)\n",
    "# tf = TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 1), tokenizer=tokenizer)\n",
    "train_tf = tf.fit_transform(X_train['text'] + X_train['keyword'] + X_train['location'] )\n",
    "val_tf = tf.transform(X_val['text'] + X_val['keyword'] + X_val['location'])\n",
    "test_tf = tf.transform(X_test['text'] + X_test['keyword'] + X_test['location'])\n",
    "train = vectorizer.fit_transform(X_train)\n",
    "val = vectorizer.transform(X_val)\n",
    "test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC(random_state=42, C=1, penalty='l2', dual=False, max_iter=1000)\n",
    "svc.fit(train_tf, y_train)\n",
    "svc.score(val_tf, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=42, \n",
    "                                n_estimators=500, \n",
    "                                min_samples_leaf=1, \n",
    "                                max_depth=500,\n",
    "                                oob_score=True)\n",
    "\n",
    "forest.fit(train_tf, y_train)\n",
    "print(forest.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "forest.score(val_tf, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "importance = sorted(zip(tf.get_feature_names(), forest.feature_importances_), key=lambda x: x[1], reverse=True)\n",
    "for imp in importance[:20]: print(\"Feature '{}', importance={}\".format(*imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "boost = AdaBoostClassifier(base_estimator=LogisticRegression(), random_state=42,\n",
    "                           n_estimators=2000, learning_rate=1)\n",
    "boost.fit(train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "boost.score(val_tf, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bagging = BaggingClassifier(base_estimator = LogisticRegression(), random_state=42,  \n",
    "                            max_features=0.7, n_jobs=-1, \n",
    "                            max_samples=1.0, n_estimators=2000)\n",
    "bagging.fit(train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bagging.score(val_tf, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device('cuda:0')\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    from torch import FloatTensor, LongTensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_loader(data, batch_size, shuffle=False):\n",
    "    features = data[0]\n",
    "    target = data[1]\n",
    "    n_samples = features.shape[0]\n",
    "    \n",
    "    indices = np.arange(n_samples)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch_indices = indices[start : end]\n",
    "        X_batch = features[batch_indices].toarray()\n",
    "        y_batch = target.values[batch_indices]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_loader(data, batch_size, shuffle=False):\n",
    "    features = data[0]\n",
    "    target = data[1]\n",
    "    n_samples = features.shape[0]\n",
    "    \n",
    "    indices = np.arange(n_samples)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch_indices = indices[start : end]\n",
    "        X_batch = features[batch_indices].toarray()\n",
    "        y_batch = target.values[batch_indices]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def data_loader(data, batch_size, shuffle=False):\n",
    "    features = data[0]\n",
    "    target = data[1]\n",
    "    n_samples = features.shape[0]\n",
    "    \n",
    "    indices = np.arange(n_samples)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch_indices = indices[start : end]\n",
    "        X_batch = features[batch_indices].toarray()\n",
    "        y_batch = target.values[batch_indices]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fit(model, loss_function, train_data=None, val_data=None, optimizer=None,\n",
    "        epoch_count=1, batch_size=1, scheduler=None, alpha=1):\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    best_model = None\n",
    "    for epoch in range(epoch_count):\n",
    "            name_prefix = '[{} / {}] '.format(epoch + 1, epoch_count)\n",
    "            epoch_train_score = 0\n",
    "            epoch_val_score = 0\n",
    "            \n",
    "            if train_data:\n",
    "                epoch_train_score = do_epoch(model, loss_function, train_data, batch_size, \n",
    "                                              optimizer, name_prefix + 'Train:', alpha=alpha\n",
    "                                            )\n",
    "                train_history.append(epoch_train_score)\n",
    "\n",
    "            if val_data:\n",
    "                name = '  Val:'\n",
    "                if not train_data:\n",
    "                    name = ' Test:'\n",
    "                epoch_val_score = do_epoch(model, loss_function, val_data, batch_size, \n",
    "                                             optimizer=None, name=name_prefix + name, alpha=alpha\n",
    "                                          )\n",
    "                \n",
    "                val_history.append(epoch_val_score)\n",
    "                if scheduler:\n",
    "                    scheduler.step(epoch_val_score)\n",
    "            elif scheduler:\n",
    "                scheduler.step(epoch_train_score)\n",
    "\n",
    "    return train_history, val_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def do_epoch(model, loss_function, data, batch_size, optimizer=None, name=None, alpha=1):\n",
    "    \"\"\"\n",
    "       Генерация одной эпохи\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    epoch_loss = 0\n",
    "   \n",
    "    batch_count = int(data[0].shape[0] / batch_size)\n",
    "   \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batch_count) as progress_bar:               \n",
    "            for ind, (X, y) in enumerate(data_loader(data, batch_size)):\n",
    "                X_batch, y_batch = FloatTensor(X).to(device), LongTensor(y).to(device)\n",
    "                \n",
    "                prediction = model(X_batch)\n",
    "                \n",
    "                loss = loss_function(prediction, y_batch)\n",
    "                \n",
    "                for param in model.children():\n",
    "                    if type(param) == nn.Linear:\n",
    "                        loss += alpha * torch.abs(param.weight).sum()\n",
    "                        \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                true_indices = torch.argmax(prediction, dim=1)\n",
    "                correct_samples = torch.sum(true_indices == y_batch).cpu().numpy()\n",
    "                accuracy += correct_samples / y_batch.shape[0]\n",
    "                if is_train:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('Epoch {} - accuracy: {:.2f}, loss {:.2f}'.format(\n",
    "                    name, (accuracy / (ind+1)), epoch_loss / (ind+1))\n",
    "                )\n",
    "            \n",
    "            accuracy /= (ind + 1)\n",
    "            epoch_loss /= (ind + 1) \n",
    "            progress_bar.set_description(f'Epoch {name} - accuracy: {accuracy:.2f}, loss: {epoch_loss:.2f}')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LinearNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NNModel():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        self.model.eval()\n",
    "        output = pd.DataFrame()\n",
    "        for ind in range(inputs.shape[0]):\n",
    "            X = FloatTensor(inputs[ind].toarray())\n",
    "            predict = self.model(X)\n",
    "            true_indices = torch.argmax(predict, dim=1).detach().cpu().numpy()\n",
    "            output.loc[ind, 'target'] = true_indices\n",
    "        return output.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fit settings\n",
    "batch_size = 100\n",
    "epoch_count = 10\n",
    "\n",
    "# optim settings\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0\n",
    "alpha = 0.005\n",
    "\n",
    "# model settings\n",
    "linear1_out = int(train_tf.shape[1]**0.5)\n",
    "output = 2\n",
    "dropout = 0.3\n",
    "\n",
    "# scheduler settings\n",
    "factor = 0.5\n",
    "patience = 3\n",
    "threshold = 1e-2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(train_tf.shape[1], linear1_out),\n",
    "                      nn.BatchNorm1d(linear1_out),\n",
    "#                       nn.Dropout(p=dropout, inplace=True),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.Linear(linear1_out, output),\n",
    "                      nn.ReLU(inplace=True)\n",
    "                     ).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "                        model.parameters(),\n",
    "                        lr=learning_rate, \n",
    "                        weight_decay=weight_decay\n",
    "                    )\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=factor, \n",
    "                              patience=patience, verbose=True, threshold=threshold\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "                        model.parameters(),\n",
    "                        lr=learning_rate, \n",
    "                        weight_decay=weight_decay\n",
    "                    )\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=factor, \n",
    "                              patience=patience, verbose=True, threshold=threshold\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(bagging)\n",
    "models.append(forest)\n",
    "models.append(boost)\n",
    "models.append(svc)\n",
    "models.append(NNModel(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def ensemble(models, data):\n",
    "    predicts = pd.DataFrame()\n",
    "    for i, model in enumerate(models):\n",
    "        predicts[i] = model.predict(data)\n",
    "    result = predicts.apply(lambda row: row.value_counts().index[0], axis=1)\n",
    "    return result.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(ensemble(models, val_tf), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score(ensemble(models, test_tf), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# choose your best model\n",
    "final_model = NNModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def submission(model, vectorizer, file_name=\"submission.csv\"):\n",
    "    test_data = pd.read_csv(data_folder + \"/test.csv\")\n",
    "    all_lemmatized_texts = lemmatize_texts(test_data.text)\n",
    "    test = vectorizer.transform(all_lemmatized_texts)\n",
    "    submit = pd.DataFrame()\n",
    "    submit['id'] = test_data['id']\n",
    "    submit['target'] = final_model.predict(test)\n",
    "    submit['target'] = submit['target'].astype('int')\n",
    "    submit.to_csv(data_folder + file_name, index=False)\n",
    "    \n",
    "submission(final_model, vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}