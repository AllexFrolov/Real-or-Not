{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import sklearn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_column(dataframe):\n",
    "    hashtags = []\n",
    "    for text in dataframe.text:\n",
    "        result = re.findall('#\\w+', text)\n",
    "        if result != []:\n",
    "            result = [w[1:].lower() for w in result]\n",
    "            hashtags.append(' '.join(result))\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_texts(texts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    result = []\n",
    "    for t in texts:\n",
    "        lemmatized_words = []\n",
    "        t = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "                  'url', t)\n",
    "        #t = re.sub('\\!+', '', t)\n",
    "        #t = re.sub('\\?+', '', t)\n",
    "        #t = re.sub('\\d+[\\:|\\.]?\\d*\\s')\n",
    "        t = re.sub('\\d+', '', t)\n",
    "        tokens = re.findall('''\\d+,\\d+|\\w+'\\w+|#?\\w+-?\\w+|\\w+\\*+\\w+''', t)\n",
    "        \n",
    "        if tokens == []:\n",
    "            print(t)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token.lower() not in stop_words:\n",
    "                lemmatized_words.append(lemmatizer.lemmatize(token).lower())\n",
    "        result.append(' '.join(lemmatized_words).replace('#', ''))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  72070\n",
      "Unique_words:  15827\n"
     ]
    }
   ],
   "source": [
    "all_lemmatized_texts = lemmatize_texts(data.text)\n",
    "all_lemmatized_tokens = [w for t in all_lemmatized_texts for w in t.split(' ')]\n",
    "print('Total words: ', len(all_lemmatized_tokens))\n",
    "print('Unique_words: ', len(set(all_lemmatized_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('url', 4114),\n",
       " ('û_', 349),\n",
       " ('like', 347),\n",
       " ('fire', 339),\n",
       " ('amp', 327),\n",
       " (\"i'm\", 248),\n",
       " ('get', 248),\n",
       " ('new', 217),\n",
       " ('one', 201),\n",
       " ('news', 199),\n",
       " ('people', 198),\n",
       " ('disaster', 159),\n",
       " ('video', 158),\n",
       " ('emergency', 156),\n",
       " ('time', 147),\n",
       " ('body', 145),\n",
       " ('day', 141),\n",
       " ('police', 141),\n",
       " ('year', 133),\n",
       " ('would', 132)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common words in dataset\n",
    "freq = nltk.probability.FreqDist(all_lemmatized_tokens)\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('url', 2228),\n",
       " ('fire', 253),\n",
       " ('û_', 172),\n",
       " ('news', 142),\n",
       " ('amp', 130),\n",
       " ('disaster', 120),\n",
       " ('california', 110),\n",
       " ('suicide', 109),\n",
       " ('police', 107),\n",
       " ('people', 106)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common words in real tweets\n",
    "real_tweets = data[data.target == 1].text\n",
    "real_tweets = lemmatize_texts(real_tweets)\n",
    "freq_real = nltk.probability.FreqDist([w for t in real_tweets for w in t.split(' ')])\n",
    "freq_real.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('url', 1886),\n",
       " ('like', 254),\n",
       " (\"i'm\", 203),\n",
       " ('amp', 197),\n",
       " ('get', 181),\n",
       " ('û_', 177),\n",
       " ('new', 163),\n",
       " ('one', 132),\n",
       " ('body', 114),\n",
       " ('would', 98)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common words in fake tweets\n",
    "fake_tweets = data[data.target == 0].text\n",
    "fake_tweets = lemmatize_texts(fake_tweets)\n",
    "freq_fake = nltk.probability.FreqDist([w for t in fake_tweets for w in t.split(' ')])\n",
    "freq_fake.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(all_lemmatized_texts, \n",
    "                                                    data.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, train_size = 0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize texts\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), tokenizer=tokenizer)\n",
    "train = vectorizer.fit_transform(X_train)\n",
    "val = vectorizer.transform(X_val)\n",
    "test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM (accuracy: 0.78)\n",
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC(random_state=0)\n",
    "# svc_parameters = {'tol' : [1e-7, 1e-6, 1e-5], \n",
    "#                   'max_iter': np.arange(400, 1000, 200)\n",
    "#                  }\n",
    "# grids_svc = GridSearchCV(svc, svc_parameters, n_jobs=-1, cv=5)\n",
    "svc.fit(train, y_train)\n",
    "# clf.score(test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grids_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grids_clf.best_score_, grids_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(random_state=42, \n",
    "                                n_estimators=71, \n",
    "                                min_samples_leaf=2, \n",
    "                                max_features=300, \n",
    "                                oob_score=True)\n",
    "forest.fit(train, y_train)\n",
    "# forest_params = {'max_features': np.arange(100, 1000, 100)}\n",
    "# grid_forest = GridSearchCV(forest, forest_params, n_jobs=-1, cv=5)\n",
    "print(forest.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# grid_forest.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# grid_forest.best_score_, grid_forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (0.7779967159277504, {'n_estimators': 71})\n",
    "# (0.790311986863711, {'max_features': 'auto'})\n",
    "# (0.7957307060755336, {'max_features': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "boost = AdaBoostClassifier(base_estimator=sklearn.tree.DecisionTreeClassifier(max_depth=10),\n",
    "                           random_state=42, algorithm='SAMME')\n",
    "boost_params = {'n_estimators': np.arange(1, 10),\n",
    "                'learning_rate': np.arange(0.01, 0.1, 0.01)\n",
    "                }\n",
    "grid_boost = GridSearchCV(boost, boost_params, n_jobs=-1, cv=5)\n",
    "# boost.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_boost.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_boost.best_score_, grid_boost.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device('cuda:0')\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    from torch import FloatTensor, LongTensor\n",
    "    \n",
    "try:\n",
    "    from google.colab import drive\n",
    "    is_in_colab = True\n",
    "except:\n",
    "    is_in_colab = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data, batch_size, shuffle=False):\n",
    "    features = data[0]\n",
    "    target = data[1]\n",
    "    n_samples = features.shape[0]\n",
    "    \n",
    "    indices = np.arange(n_samples)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch_indices = indices[start : end]\n",
    "        X_batch = features[batch_indices].toarray()\n",
    "        y_batch = target.values[batch_indices]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, loss_function, train_data=None, val_data=None, optimizer=None,\n",
    "        epoch_count=1, batch_size=1, scheduler=None, alpha=1):\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    best_model = None\n",
    "    for epoch in range(epoch_count):\n",
    "            name_prefix = '[{} / {}] '.format(epoch + 1, epoch_count)\n",
    "            epoch_train_score = 0\n",
    "            epoch_val_score = 0\n",
    "            \n",
    "            if train_data:\n",
    "                epoch_train_score = do_epoch(model, loss_function, train_data, batch_size, \n",
    "                                              optimizer, name_prefix + 'Train:', alpha=alpha\n",
    "                                            )\n",
    "                train_history.append(epoch_train_score)\n",
    "\n",
    "            if val_data:\n",
    "                name = '  Val:'\n",
    "                if not train_data:\n",
    "                    name = ' Test:'\n",
    "                epoch_val_score = do_epoch(model, loss_function, val_data, batch_size, \n",
    "                                             optimizer=None, name=name_prefix + name, alpha=alpha\n",
    "                                          )\n",
    "                \n",
    "                val_history.append(epoch_val_score)\n",
    "                if scheduler:\n",
    "                    scheduler.step(epoch_val_score)\n",
    "            elif scheduler:\n",
    "                scheduler.step(epoch_train_score)\n",
    "\n",
    "    return train_history, val_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loss_function, data, batch_size, optimizer=None, name=None, alpha=1):\n",
    "    \"\"\"\n",
    "       Генерация одной эпохи\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    epoch_loss = 0\n",
    "   \n",
    "    batch_count = int(data[0].shape[0] / batch_size)\n",
    "   \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batch_count) as progress_bar:               \n",
    "            for ind, (X, y) in enumerate(data_loader(data, batch_size)):\n",
    "                X_batch, y_batch = FloatTensor(X).to(device), LongTensor(y).to(device)\n",
    "                \n",
    "                prediction = model(X_batch)\n",
    "                \n",
    "                loss = loss_function(prediction, y_batch)\n",
    "                \n",
    "                for param in model.children():\n",
    "                    if type(param) == nn.Linear:\n",
    "                        loss += alpha * torch.abs(param.weight).sum()\n",
    "                        \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                true_indices = torch.argmax(prediction, dim=1)\n",
    "                correct_samples = torch.sum(true_indices == y_batch).cpu().numpy()\n",
    "                accuracy += correct_samples / y_batch.shape[0]\n",
    "                if is_train:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('Epoch {} - accuracy: {:.2f}, loss {:.2f}'.format(\n",
    "                    name, (accuracy / (ind+1)), epoch_loss / (ind+1))\n",
    "                )\n",
    "            \n",
    "            accuracy /= (ind + 1)\n",
    "            epoch_loss /= (ind + 1) \n",
    "            progress_bar.set_description(f'Epoch {name} - accuracy: {accuracy:.2f}, loss: {epoch_loss:.2f}')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LinearNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fit settings\n",
    "batch_size = 100\n",
    "epoch_count = 10\n",
    "\n",
    "# optim settings\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0\n",
    "alpha = 0.005\n",
    "\n",
    "# model settings\n",
    "linear1_out = int(train.shape[1]**0.5)\n",
    "output = 2\n",
    "\n",
    "# scheduler settings\n",
    "factor = 0.5\n",
    "patience = 1\n",
    "threshold = 1e-2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(train.shape[1], linear1_out),\n",
    "                      nn.BatchNorm1d(linear1_out),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.Linear(linear1_out, output),\n",
    "                      nn.ReLU(inplace=True)\n",
    "                     ).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "                        model.parameters(),\n",
    "                        lr=learning_rate, \n",
    "                        weight_decay=weight_decay\n",
    "                    )\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=factor, \n",
    "                              patience=patience, verbose=True, threshold=threshold\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 10] Train: - accuracy: 0.71, loss: 20.50: : 49it [00:01, 29.85it/s]                     \n",
      "Epoch [1 / 10]   Val: - accuracy: 0.74, loss: 9.91: : 13it [00:00, 31.04it/s]                     \n",
      "Epoch [2 / 10] Train: - accuracy: 0.75, loss: 9.07: : 49it [00:01, 29.97it/s]                     \n",
      "Epoch [2 / 10]   Val: - accuracy: 0.77, loss: 8.59: : 13it [00:00, 30.38it/s]                     \n",
      "Epoch [3 / 10] Train: - accuracy: 0.76, loss: 8.26: : 49it [00:02, 23.95it/s]                     \n",
      "Epoch [3 / 10]   Val: - accuracy: 0.76, loss: 8.03: : 13it [00:00, 30.78it/s]                     \n",
      "Epoch [4 / 10] Train: - accuracy: 0.77, loss: 7.94: : 49it [00:02, 24.31it/s]                     \n",
      "Epoch [4 / 10]   Val: - accuracy: 0.76, loss: 7.85: : 13it [00:00, 30.45it/s]                     \n",
      "Epoch [5 / 10] Train: - accuracy: 0.74, loss 6.96:   8%|▊         | 4/48 [00:00<00:01, 24.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     4: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5 / 10] Train: - accuracy: 0.80, loss: 4.99: : 49it [00:02, 24.26it/s]                     \n",
      "Epoch [5 / 10]   Val: - accuracy: 0.78, loss: 4.43: : 13it [00:00, 29.69it/s]                     \n",
      "Epoch [6 / 10] Train: - accuracy: 0.81, loss: 4.47: : 49it [00:02, 24.41it/s]                     \n",
      "Epoch [6 / 10]   Val: - accuracy: 0.78, loss: 4.52: : 13it [00:00, 30.82it/s]                     \n",
      "Epoch [7 / 10] Train: - accuracy: 0.78, loss 4.09:   8%|▊         | 4/48 [00:00<00:01, 23.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     6: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7 / 10] Train: - accuracy: 0.84, loss: 2.92: : 49it [00:02, 24.41it/s]                     \n",
      "Epoch [7 / 10]   Val: - accuracy: 0.78, loss: 2.54: : 13it [00:00, 30.45it/s]                     \n",
      "Epoch [8 / 10] Train: - accuracy: 0.84, loss: 2.55: : 49it [00:02, 24.17it/s]                     \n",
      "Epoch [8 / 10]   Val: - accuracy: 0.79, loss: 2.69: : 13it [00:00, 31.04it/s]                     \n",
      "Epoch [9 / 10] Train: - accuracy: 0.85, loss: 2.61: : 49it [00:02, 24.09it/s]                     \n",
      "Epoch [9 / 10]   Val: - accuracy: 0.77, loss: 2.67: : 13it [00:00, 30.31it/s]                     \n",
      "Epoch [10 / 10] Train: - accuracy: 0.85, loss: 2.61: : 49it [00:02, 23.97it/s]                     \n",
      "Epoch [10 / 10]   Val: - accuracy: 0.78, loss: 2.74: : 13it [00:00, 30.53it/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    10: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, _ = fit(model, loss_function, train_data=(train, y_train), \n",
    "           optimizer=optimizer, batch_size=100, epoch_count=epoch_count,\n",
    "           alpha = alpha, val_data=(val, y_val), scheduler=scheduler\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(vectorizer):\n",
    "    test_data = pd.read_csv(\"data/test.csv\")\n",
    "    all_lemmatized_texts = lemmatize_texts(test_data.text)\n",
    "    test = vectorizer.transform(all_lemmatized_texts)\n",
    "    model.eval()\n",
    "    submis = pd.DataFrame()\n",
    "    submis['id'] = test_data['id']\n",
    "    for ind in range(test.shape[0]):\n",
    "        X = FloatTensor(test[ind].toarray())\n",
    "        predict = model(X)\n",
    "        true_indices = torch.argmax(predict, dim=1).detach().cpu().numpy()\n",
    "        submis.loc[ind, 'target'] = true_indices\n",
    "    submis['target'] = submis['target'].astype('int')\n",
    "    submis.to_csv(\"data/submission_test.csv\", index=False)\n",
    "    \n",
    "submission(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
